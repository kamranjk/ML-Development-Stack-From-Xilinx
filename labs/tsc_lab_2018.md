# Xilinx  ML Suite TSC Lab 2018

## Introduction
Welcome to the ML Suite TSC Lab. By going through this lab, you will get hands on experience with using the Xilinx ML Suite. This lab will cover how to use the Python APIs for deploying the included models, and how to compile and quantize custom models with the xfDNN Python tools.

The lab is in two parts:
1. Part 1: Using the Python APIs to run the included Image Classification examples with GoogLeNetv1 and pre-quantized 8bit model.
2. Part 2: Use the xfDNN tools to compile and quantize a custom model to be used with the Image Classification example in Part 1.

This lab will follow tutorials from the [Xilinx ML Suite][].

## Launching Your ML Suite Instance

something, something, something darkside....

## Part 1 - Image Classification with Python APIs
In the first part, you will run through the [Batch Classification example][], using the xfDNN Python APIs.

Directory overview of this tutorial:

<pre>
pyxdnn/
└── examples
    ├── batch_classify
    │   ├── batch_classify.py					
    │   ├── images
    │   ├── images_224
    │   ├── run.sh
    │   ├── tsc.sh											
    │   └── synset_words.txt
    ├── common
    └── multinet
        ├── mytest.json
        ├── run.sh
        ├── synset_words.txt
        └── test_classify_async_multinet.py
</pre>

1. Navigate to /home/centos/xfdnn_18_04_02/pyxdnn/examples/batch_classify/

	```bash
	$ cd xfdnn_18_04_02/pyxdnn/examples/batch_classify/
	$ ls
	batch_classify.py  images  images_224  run.sh  synset_words.txt  lab_part1.sh lab_part2.sh
	```

2. Open `lab_part1.sh` with the text editor of your choice. These labs will use nano, as its a simple editor. When using nano, remember to use `sudo` to launch it, so you can save your edits. Execute `sudo nano lab_part1.sh` to view the contents of the script.

	The top of the file includes environment and setup commands. No changes are needed here.  
	```sh
	#!/usr/bin/env bash

	if [ "$EUID" -ne 0 ]
	  then echo "Please run as root"
	  exit
	fi

	export PYXDNN_ROOT=../common
	export LD_LIBRARY_PATH=$PYXDNN_ROOT/runtime/lib/x86_64:$PYXDNN_ROOT/lib
	export XILINX_OPENCL=$PYXDNN_ROOT
	source $PYXDNN_ROOT/setup.sh
	```
	Beginning on line 14, you will see the python command and parameters needed to run the Image Classification example.
	```sh
	# --------------------------------------------------------------------
	# Part 1: Running GoogleNetv1 with Standard 1000 class model
	# -------------------------------------------------------------------

	echo "Image Classification with GoogleNetv1 with 8 bit model"

	/usr/bin/python batch_classify.py \
	--xclbin $PYXDNN_ROOT/kernel_8b.xclbin \
	--xlnxnet $PYXDNN_ROOT/xdnn_scheduler/googlenet.fpgaaddr.64.txt \
	--fpgaoutsz 1024 \
	--datadir $PYXDNN_ROOT/data_googlenet_v1 \
	--labels synset_words.txt \
	--xlnxlib $PYXDNN_ROOT/lib/libxblas.so \
	--imagedir images_224 \
	--useblas \
	--xlnxcfg $PYXDNN_ROOT/xdnn_scheduler/googlenet_quantized.json

	```

	This example runs eight images through the Python API in batch or streaming mode.

	The `lab_part1.sh` example calls the API, and takes the following parameters:
	batch_classify.py
	- `--xclbin` 		- Defines which FPGA binary to use. By Default, leave set to the binary in the example
	- `--xlnxcfg` 	- FPGA config file
	- `--xlnxnet` 	- FPGA instructions generated by xfDNN Compiler for the network
	- `--fpgaoutsz`	- Size of FPGA output blob
	- `--datadir`		- Path to data files to run for the network (weights)
	- `--labels`		- Result -> labels translation file (typically Text File)
	- `--xlnxlib`		- FPGA xfDNN lib
	- `--imagedir`	- Directory with image files to classify
	- `--useblas`		- Use BLAS-optimized functions (requires xfDNN lib compiled with BLAS)

	Exit the file, once you have become familiar with the script and its components.

3. Execute the example, by running `sudo ./lab_part1.sh`. The

	```sh
	$ sudo ./lab_part1.sh
	Image Classification with GoogleNetv1 with 8 bit model
	[XBLAS] # kernels: 1
	[XDNN] using custom DDR banks 0,2,1,1
	Device/Slot[0] (/dev/xdma0, 0:0:1d.0)
	xclProbe found 1 FPGA slots with XDMA driver running
	[time] loadImageBlobFromFile/OpenCV (4.15 ms):
	[time] loadImageBlobFromFile/OpenCV (2.13 ms):
	[time] loadImageBlobFromFile/OpenCV (1.91 ms):
	[time] loadImageBlobFromFile/OpenCV (1.45 ms):
	[time] loadImageBlobFromFile/OpenCV (1.56 ms):
	[time] loadImageBlobFromFile/OpenCV (1.57 ms):
	[time] loadImageBlobFromFile/OpenCV (1.60 ms):
	[time] loadImageBlobFromFile/OpenCV (2.19 ms):
	[time] prepareImages (24.43 ms):
	CL_PLATFORM_VENDOR Xilinx
	CL_PLATFORM_NAME Xilinx
	CL_DEVICE_0: 0x27a6e30
	CL_DEVICES_FOUND 1, using 0
	loading ../common/kernel_8b.xclbin
	[XBLAS] kernel0: kernelSxdnn_0
	Loading weights/bias/quant_params to FPGA...
	WARNING: unaligned host pointer detected, this leads to extra memcpy
	WARNING: unaligned host pointer detected, this leads to extra memcpy
	WARNING: unaligned host pointer detected, this leads to extra memcpy
	WARNING: unaligned host pointer detected, this leads to extra memcpy
	WARNING: unaligned host pointer detected, this leads to extra memcpy
	WARNING: unaligned host pointer detected, this leads to extra memcpy
	WARNING: unaligned host pointer detected, this leads to extra memcpy
	WARNING: unaligned host pointer detected, this leads to extra memcpy
	[XDNN] FPGA metrics (0/0/0)
	[XDNN]   write_to_fpga  : 0.17 ms
	[XDNN]   exec_xdnn      : 39.57 ms
	[XDNN]   read_from_fpga : 0.06 ms
	[XDNN] FPGA metrics (0/1/0)
	[XDNN]   write_to_fpga  : 0.15 ms
	[XDNN]   exec_xdnn      : 39.42 ms
	[XDNN]   read_from_fpga : 0.06 ms
	[XDNN] FPGA metrics (0/2/0)
	[XDNN]   write_to_fpga  : 0.15 ms
	[XDNN]   exec_xdnn      : 38.95 ms
	[XDNN]   read_from_fpga : 0.08 ms
	[XDNN] FPGA metrics (0/3/0)
	[XDNN]   write_to_fpga  : 0.17 ms
	[XDNN]   exec_xdnn      : 43.37 ms
	[XDNN]   read_from_fpga : 0.05 ms
	[time] FPGA xdnn execute (580.57 ms):
	[time] FC (2.00 ms):

	---------- Prediction 0 for images_224/cat gray.jpg ----------
	0.8056 - "n02123394 Persian cat"
	0.1379 - "n02127052 lynx, catamount"
	0.0165 - "n02326432 hare"
	0.0130 - "n02120079 Arctic fox, white fox, Alopex lagopus"
	0.0098 - "n02123159 tiger cat"

	---------- Prediction 1 for images_224/cat.jpg ----------
	0.9730 - "n02123159 tiger cat"
	0.0164 - "n02124075 Egyptian cat"
	0.0036 - "n02123045 tabby, tabby cat"
	0.0020 - "n02127052 lynx, catamount"
	0.0014 - "n02119789 kit fox, Vulpes macrotis"

	---------- Prediction 2 for images_224/ILSVRC2012_val_00003225.JPEG ----------
	0.9033 - "n02422106 hartebeest"
	0.0780 - "n02423022 gazelle"
	0.0183 - "n02422699 impala, Aepyceros melampus"
	0.0001 - "n02408429 water buffalo, water ox, Asiatic buffalo, Bubalus bubalis"
	0.0001 - "n02437616 llama"

	---------- Prediction 3 for images_224/ILSVRC2012_val_00001172.JPEG ----------
	0.4223 - "n02114548 white wolf, Arctic wolf, Canis lupus tundrarum"
	0.3238 - "n02109961 Eskimo dog, husky"
	0.2243 - "n02114367 timber wolf, grey wolf, gray wolf, Canis lupus"
	0.0200 - "n02110063 malamute, malemute, Alaskan malamute"
	0.0073 - "n02110185 Siberian husky"

	---------- Prediction 4 for images_224/fish-bike.jpg ----------
	0.5581 - "n02797295 barrow, garden cart, lawn cart, wheelbarrow"
	0.3857 - "n04482393 tricycle, trike, velocipede"
	0.0271 - "n02835271 bicycle-built-for-two, tandem bicycle, tandem"
	0.0127 - "n04258138 solar dish, solar collector, solar furnace"
	0.0020 - "n03599486 jinrikisha, ricksha, rickshaw"

	---------- Prediction 5 for images_224/cat_gray.jpg ----------
	0.8056 - "n02123394 Persian cat"
	0.1379 - "n02127052 lynx, catamount"
	0.0165 - "n02326432 hare"
	0.0130 - "n02120079 Arctic fox, white fox, Alopex lagopus"
	0.0098 - "n02123159 tiger cat"

	---------- Prediction 6 for images_224/cat gray.jpg ----------
	0.8056 - "n02123394 Persian cat"
	0.1379 - "n02127052 lynx, catamount"
	0.0165 - "n02326432 hare"
	0.0130 - "n02120079 Arctic fox, white fox, Alopex lagopus"
	0.0098 - "n02123159 tiger cat"

	---------- Prediction 7 for images_224/cat.jpg ----------
	0.9730 - "n02123159 tiger cat"
	0.0164 - "n02124075 Egyptian cat"
	0.0036 - "n02123045 tabby, tabby cat"
	0.0020 - "n02127052 lynx, catamount"
	0.0014 - "n02119789 kit fox, Vulpes macrotis"

	Num processed: 8

	[time] Total loop (9748.08 ms):

	```
4. Take a look at the performance results here. The `[time] loadImageBlobFromFile/OpenCV (4.15 ms):` messages show the time it takes for formatting the images on the cpu to prepare them for Image Classification. This is done one time at the beginning of the run.

	The next important read out is the processing time on the xDNN kernel. Look for the following message:
	```sh
	[XDNN] FPGA metrics (0/0/0)
	[XDNN]   write_to_fpga  : 0.17 ms
	[XDNN]   exec_xdnn      : 39.57 ms
	[XDNN]   read_from_fpga : 0.06 ms
	```
Here its showing the write, execution and read times from each of the 4 xDNN kernels on the FPGA. This example runs a default of 8 images, 2 per kernel. Because each kernel needs to be initialized for the network/model to be run, it has a high processing time. Feeding more images to it, will show the real processing time. For example, if you change this to run more images, you will see the following results:
	```sh
	[XDNN] FPGA metrics (0/0/0)
	[XDNN]   write_to_fpga  : 0.21 ms
	[XDNN]   exec_xdnn      : 5.27 ms
	[XDNN]   read_from_fpga : 0.06 ms
	```


## Part 2: Image Classification with Custom models
In this part, you will take a look at how to use a custom model with Image Classification GoogLeNetv1 example from above. Here you will use the same network, but will provide a new model. For this, you will first compile the network again, then quantize a different 32bit model and execute with the same Python APIs. The model this lap will use is FLowers102, but any custom model trained from GoogLeNetv1 would work here.

This part will use the same pyXDNN dir:

```sh
pyxdnn/
└── examples
    ├── batch_classify
    │   ├── batch_classify.py
    │   ├── images
    │   ├── images_224
    │   ├── run.sh
    │   ├── tsc.sh
    │   └── synset_words.txt
    ├── common
    └── multinet
        ├── mytest.json
        ├── run.sh
        ├── synset_words.txt
        └── test_classify_async_multinet.py
```

In this section you will use the xfDNN Tools located in the `/xfdnn_18_04_02/xfdnn_tools/` dir.
```sh
xfdnn_tools/
├── compile
│   ├── codegeneration
│   ├── docs
│   │   ├── caffee.pdf
│   │   ├── DDR.pdf
│   │   ├── networks.pdf
│   │   ├── rules.pyc
│   │   ├── rules.txt
│   │   └── tensorflow.pdf
│   ├── graph
│   ├── memory
│   ├── network
│   ├── notes.sh
│   ├── optimizations
│   ├── README.mxnet
│   ├── README.rst
│   ├── scripts
│   ├── tests
│   │   ├── fullmonty.pyc
│   │   ├── mxnet.pyc
│   │   ├── xfdnn_compiler_inflammable.pyc
│   │   ├── xfdnn_compiler.pyc
│   │   └── xfdnn_compiler_tensorflow.pyc
│   ├── version
│   └── weights
├── models
└── quantize
    ├── quantize_base.pyc
    ├── quantize_caffe.pyc
    ├── quantize.pyc
    └── run_quantize.sh
```

This part will also look at other included models in the `/xfdnn_18_04_02/models/` dir. In the models dir, you have access to 4 models: GoogLeNetv1, ResNet50, Flowers102 and Places365. Each model comes with the original fp32 model along with pre-quantized 16bit and 8bit versions. In this example, you will recompile and quantize the flowers102 model from the original fp32 model and deploy it using the Python APIs from part 1.

```sh
/xfdnn_18_04_02/models/
├── bvlc_googlenet_without_lrn
├── flowers102
│   ├── fp32
│   │   ├── bvlc_googlenet_without_lrn.caffemodel
│   │   ├── bvlc_googlenet_without_lrn_deploy.prototxt
│   │   ├── ...
│   ├── int16
│   └── int8
├── googlenet_v1
├── places365
└── resnet
```

Ready? Let's begin.

1. Navigate to `/home/centos/xfdnn_18_04_02/caffe/`
	```
	$ ls
	classification.bin  kernelSxdnn_hw_f1_16b.xclbin  run_common.sh         run_places_16b.sh  xdnn_scheduler
	data                kernelSxdnn_hw_f1_8b.xclbin   run_cpu_env.sh        run_resnet_16b.sh  xlnx-docker
	demo                libs                          run_flowers_16b.sh    run_resnet_8b.sh   xlnx-xdnn-f1
	examples            models                        run_googlenet_16b.sh  sdaccel.ini
	execDocker.sh       README                        run_googlenet_8b.sh   start_docker.sh
	```

2. Execute `./start_docker.sh` to enter application docker.
	```sh
	$ ./start_docker.sh
	/opt#
	```

3. Set XFDNN_ROOT to /xlnx
	```
	export XFDNN_ROOT=/xlnx/xfdnn_tools/compile/
	```

4. Navigate to `/xlnx/xfdnn_tools/compile/`
	```
	# cd /xlnx/xfdnn_tools/compile/
	```

5. This next command will execute the GoogLeNet-v1 compiler using a prototxt for caffe. Here you will pass the GoogLeNetv1 prototxt along with the caffemodel from the `models/flowers102` dir. Note: In step 2, you entered the caffe container, which is needed to run the xfDNN tools. Outside the container the dir structure is `/home/centos/xfdnn_18_04_02/...` in the container this maps to `/xlnx/`

    The compiler version you will use is `xf_dnn_compiler_inflamable.pyc`. The parameters are:
    - `[-n,]` - Input prototxt for compiler
    - `[-s,]` - Strategies for compiler (default: all)
    - `[-m,]` - On-chip Memory available in MB (default: 4)
    - `[-i,]` - xfDNN kernel dimension (28 or 56, default: 28)
    - `[-w,--weights WEIGHTS]` -  Caffe trained model (optional)
    - `[-g,]` -  Output commands in XFDNN test and json
    - `[-o,]` -  Output png file of graph read by compiler (optional)

    For the `-n` and `-w` use the deploy prototxt and caffemodel from the `/flowers102` dir. (reference the dir tree at the beginning of part 2)

    `-s`, `-m`, `-i` will be set to the default values of `all`, `4` and `28`.

    `-g` This is the output of the compiler and needed for use with the Python APIs for deployment. You can give this file a name, and save it back in the `modes/flowers102` dir so we can find it later.

    `-o` This will produce a picture of the graph for reference. Its an optional step. Give this a name as well, in the `/models/flowers102` dir.

    Execute your python command with the above arguments set:
    ```shell
    /xlnx/xfdnn_tools/compile# python tests/xfdnn_compiler_inflammable.pyc /
    -n /xlnx/models/flowers102/fp32/bvlc_googlenet_without_lrn_deploy.prototxt \
    -s all -m 4 -i 28 \
    -g /xlnx/models/flowers102/flowers102_googlenet.cmd \
    -w /xlnx/models/flowers102/fp32/bvlc_googlenet_without_lrn.caffemodel \
    -o /xlnx/models/flowers102/flowers102_graph.png
    ```

    You will get a long string of console messages while the compiler is running, but at the end you should see `SUCCESS`. If you don't and error message will explain if you had any problems with your arguments.

    To verify the success of this step, check the `models/flowers102/` dir for the generated files:

    ```sh
    /xlnx/xfdnn_tools/compile# cd /xlnx/models/flowers102/
    /xlnx/models/flowers102# ls    
    flowers102_googlenet.cmd  flowers102_googlenet.cmd.json  flowers102_googlenet.cmd.out  flowers102_graph.png  fp32  int16  int8  original.png
    /xlnx/models/flowers102#
    ```

    Confirm the files `flowers102_googlenet.cmd` and `flowers102_graph.png` are there.


6. Now you will qunatize the fp32 flowers102 model to int8. Navigate to `/xlnx/xfdnn_tools/quantize/`
  	```sh
    /xlnx/models/flowers102# cd /xlnx/xfdnn_tools/quantize/
    /xlnx/xfdnn_tools/quantize# ls
    quantize.pyc  quantize_base.pyc  quantize_caffe.pyc  run_quantize.sh
    /xlnx/xfdnn_tools/quantize#
  	```

7. In this lab, you will use the `qunatize.pyc` tool. The parameters are:
    - `--deploy_model` - This input will be the prototxt for the network/model you want to quantize.
    - `--weights` - This input will the fp32  caffemodel.
    - `--calibration_directory` - This is where the original data set is located, which was used to train the model.
    - `--calibration_size` - This input is the number of images you want to provide for the calibration process.

    `--deploy_model` and `--weights` will be the same from step 5, located in the `models/flowers/` dir.

    For the original images needed for calibration, they are located in `/xlnx/imagenet_val`.

    `calibration_size` can be set to a default `8`.

    Execute your python command with the above arguments set:
  	```sh
    /xlnx/xfdnn_tools/quantize# python quantize.pyc \
    --deploy_model /xlnx/models/flowers102/fp32/bvlc_googlenet_without_lrn_deploy.prototxt \
    --weights /xlnx/models/flowers102/fp32/bvlc_googlenet_without_lrn.caffemodel \
    --calibration_directory ../../imagenet_val/ \
    --calibration_size 8
  	```

	If this runs successfully the end of the console messages you will see:
    ```sh
    Passing
    Writing output files to /xlnx/models/flowers102/fp32/bvlc_googlenet_without_lrn_deploy.json...
    Arguments:
    --deploy_model /xlnx/models/flowers102/fp32/bvlc_googlenet_without_lrn_deploy.prototxt
    --weights /xlnx/models/flowers102/fp32/bvlc_googlenet_without_lrn.caffemodel
    --calibration_directory ../../imagenet_val/
    --calibration_size 8
    --calibration_seed None
    --calibration_indices None
    --bitwidths [8,8,8]
    --dims [3,224,224]
    --transpose [2,0,1]
    --channel_swap [2,1,0]
    --raw_scale 255.0
    --mean_value [104.0,117.0,123.0]
    --input_scale 1.0
    ```
    This will recap the quatization run, and will share where the output is located `Writing output files to /xlnx/models/flowers102/fp32/bvlc_googlenet_without_lrn_deploy.json.` This JSON file will be needed for deploying the quantized model.

    It will also store the new weights where the original model was located. For this example that is in the `../fp32/` dir and the new weights data dir will be appended with the suffix `_data`. Verify that `../fp32/bvlc_googlenet_without_lrn.caffemodel_data/` is now created. This will the path that is provided to the Python APIs for deployment.

    Congrats! You have now compiled and quantized your custom network/model for deployment.

8. Exit the caffe docker now and navigate back to `/pyxdnn/examples/batch_classify/`
    ```sh
    /xlnx/xfdnn_tools/quantize# exit
    exit
    [caffe]$ cd ../pyxdnn/examples/batch_classify/
    ```

9. Open `lab_part2.sh` script. Its the same Pyhton API from Part 1, but now you will add in the pointers to the outputs generated from the xfDNN tools, and flower images/labels to test the new model.

10. Set your working dir where all of your xfDNN outputs are located, to simplify references to them. Throughput this lab, it has `/home/centos/xfdnn_18_04_02/models/flowers102`.
    ```sh
    # Set Working Dir Path
    export MODELS=/home/centos/xfdnn_18_04_02/models/flowers102
    ```

11. Below is what you should see next. There are comments to help provide the right input parameters. The default values are already filled in. The parameters you need, but you didn't generate are:

    - `-outsz`    - For flowers102 the number of categories is `102`
    - `--labels`  - Labels are located here: `../../caffe/data/flowers102/synset_words.txt`
    - `--imagedir`- For test images of flowers, use this dir: `../../caffe/examples/flowers`

    ```sh
    # --------------------------------------------------------------------
    # Part 2: Running GoogleNetv1 with Custom model
    # -------------------------------------------------------------------


    echo " "
    echo "----------------------------------------------------------"
    echo "Image Classification with GoogleNetv1 with Custom 8 bit model"
    echo "----------------------------------------------------------"
    echo " "
    echo " "

    /usr/bin/python batch_classify.py \
    --xclbin $PYXDNN_ROOT/kernel_8b.xclbin \		#8bit 4 kernel binary. Leave as default
    --xlnxnet $MODELS/ \				                #Output of the Compiler ".cmd"
    --fpgaoutsz 1024 \                          #Parameter for final layer. Leave as default
    --outsz  \								                  #Number of Categories
    --datadir $MODELS/ \	                      #Quantized weights data dir
    --labels $MODELS/ \		                      #Text File for Labels
    --xlnxlib $PYXDNN_ROOT/lib/libxblas.so \		#Xilinx Library. Leave as default.
    --imagedir $MODELS/ \			                  #Images dir to classify
    --useblas \								                  #Flag for Blas. Leave as default.
    --xlnxcfg $MODELS/		                      #Quantization data ".json"

    ```


12. Once complete, save and exit `lab_part2.sh`. Now execute your run with `sudo ./lab_part2.tsc`. If it runs successfully you should the classification results for a number of flowers, which looks like this:
    ```sh
    ---------- Prediction 0 for /home/centos/xfdnn_18_04_02/models/flowers102/../../caffe/examples/flowers/dahlia_03000.jpg
    1.0000 - "pink-yellow dahlia"
    0.0000 - "thorn apple"
    0.0000 - "bearded iris"
    0.0000 - "geranium"
    0.0000 - "petunia"
    ```
    If you are getting errors, review your parameters to make sure they are correct. If you can't figure it out, take a look at the `lab_key.sh` script file.





### Congrats! You have compiled and quantized a model and used the Python APIs to deploy it.



[here]: tutorials/launching_instance.md
[compiler]: tutorials/compile.md
[quantizer]: tutorials/quantize.md
[Xilinx ML Suite]: https://github.com/Xilinx/ML-Suite
[Batch Classification example]: https://github.com/Xilinx/ML-Suite/blob/master/pythonexample.md
